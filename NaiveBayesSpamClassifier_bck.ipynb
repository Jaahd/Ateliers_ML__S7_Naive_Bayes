{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Spam Classification\n",
    "\n",
    "\n",
    "Naive bayes is a relatively simple probabilistic classfication algorithm that is well suitable for categorical data .\n",
    "\n",
    "In machine learning, common application of Naive Bayes are spam email classification, sentiment analysis, document categorization. Naive bayes is advantageous over other commonly used classification algorithms in its simplicity, speed, and its accuracy on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "We will be using a data from the UCI machine learning repository that countains several Youtube comments from very popular music videos. Each comment in the data has been labeled as either spam or ham (legitimate comment), we will use this data to train our Naive Bayes algorithm for youtube comment spam classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# For matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# For regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+447935454150 lovely girl talk to me xxx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I always end up coming back to this song&lt;br /&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my sister just received over 6,500 new &lt;a rel=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cool</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello I am from Palastine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0           +447935454150 lovely girl talk to me xxx      1\n",
       "1     I always end up coming back to this song<br />      0\n",
       "2  my sister just received over 6,500 new <a rel=...      1\n",
       "3                                               Cool      0\n",
       "4                          Hello I am from Palastine      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data using pandas\n",
    "data_comments = pd.read_csv('YoutubeCommentsSpam.csv')\n",
    "\n",
    "# Create column labels: \n",
    "data_comments.columns = [\"content\",\"label\"]\n",
    "data_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have two variables in our data, the youtube comment and its label of being spam or legitimate. We already know that a legitimate comment would be one relating to the video itself, let's get a sense of what spam comments look like. $\\textbf{WARNING: Please DO NOT go on the links in the spam comments below as they might be unsafe.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                +447935454150 lovely girl talk to me xxx\n",
      "2       my sister just received over 6,500 new <a rel=...\n",
      "4                               Hello I am from Palastine\n",
      "6       Go check out my rapping video called Four Whee...\n",
      "8                           Aslamu Lykum... From Pakistan\n",
      "10                            Help me get 50 subs please \n",
      "12      Alright ladies, if you like this song, then ch...\n",
      "15      <a href=\"https://www.facebook.com/groups/10087...\n",
      "16                  Take a look at this video on YouTube:\n",
      "17                 Check out our Channel for nice Beats!!\n",
      "19                    Check out this playlist on YouTube:\n",
      "21                                            like please\n",
      "24      I shared my first song &quot;I Want You&quot;,...\n",
      "25      Come and check out my music!Im spamming on loa...\n",
      "26                    Check out this playlist on YouTube:\n",
      "27      HUH HYUCK HYUCK IM SPECIAL WHO S WATCHING THIS...\n",
      "30      Check out this video on YouTube:<br /><br />Lo...\n",
      "33                    Check out this playlist on YouTube:\n",
      "34                       Check out this video on YouTube:\n",
      "35                       Check out this video on YouTube:\n",
      "38      Check out this playlist on YouTube:chcfcvzfzfb...\n",
      "39                   Check out this playlist on YouTube: \n",
      "40      Im gonna share a little ryhme canibus blows em...\n",
      "41                       Check out this video on YouTube:\n",
      "42      Check out this video on YouTube<br /><br /><br />\n",
      "43        CHECK OUT THE NEW REMIX !!!<br />CLICK CLICK !!\n",
      "44                    Check out this playlist on YouTube:\n",
      "45      I personally have never been in a abusive rela...\n",
      "48                                  plese subscribe to me\n",
      "49                       Check out this video on YouTube:\n",
      "                              ...                        \n",
      "1915             CHECK OUT partyman318 FR GOOD TUNEZ!! :D\n",
      "1916    Hey youtubers... I really appreciate all of yo...\n",
      "1917    Hey Music Fans I really appreciate any of you ...\n",
      "1918    Hey Music Fans I really appreciate any of you ...\n",
      "1919    Hey Music Fans I really appreciate any of you ...\n",
      "1920                   Hi. Check out and share our songs.\n",
      "1921                   Hi. Check out and share our songs.\n",
      "1922                    Hi.Check out and share our songs.\n",
      "1923    Hey Music Fans I really appreciate any of you ...\n",
      "1924    Hey, I am doing the Forty Hour famine so I ll ...\n",
      "1925             Love itt and ppl check out my channel!!!\n",
      "1926                                 SUBSCRIBE MY CHANNEL\n",
      "1927                                       adf.ly / KlD3Y\n",
      "1928                                       adf.ly / KlD3Y\n",
      "1929                               check out my new video\n",
      "1930    Hey Music Fans I really appreciate all of you ...\n",
      "1931    Hello everyone, It Is not my intention to spam...\n",
      "1932    ******* Facebook is LAME and so 2004! Check ou...\n",
      "1933    Please check out and send to others Freedom an...\n",
      "1934    Nice to meet You - this is Johnny: 1. If You a...\n",
      "1935     hey you ! check out the channel of Alvar Lake !!\n",
      "1936    Hi -this is Johnny: 1. If You already know my ...\n",
      "1940    Check out this video on YouTube:<br />&quot;Th...\n",
      "1942    O peoples of the earth, I have seen how you pe...\n",
      "1945    I WILL NEVER FORGET THIS SONG IN MY LIFE LIKE ...\n",
      "1946    ********OMG Facebook is OLD! Check out  ------...\n",
      "1947    Hey Music Fans I really appreciate all of you ...\n",
      "1948    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1949    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "1950    **CHECK OUT MY NEW MIXTAPE**** **CHECK OUT MY ...\n",
      "Name: content, Length: 1004, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show spam comments in data\n",
    "# DO NOT GO ON THE LINKS BELOW!!!\n",
    "print(data_comments[\"content\"][data_comments[\"label\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browsing over the comments that have been labeled as spam in this data, it seems like these comments are either unrelated to the video, or are some form of advertisement. The phrase \"check out\" seems to be very popular in this comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics and Data Cleaning\n",
    "\n",
    "The table below shows that this data set consist of $1959$ youtube comments, about $49\\%$ of them are legitimate comments and about $51\\%$ are spam. This high variation of classes in our data set will help us test our algorithms accuracy on the test data set. The average length of each comment is about $96$ characters, which is roughly about $15$ words on average per comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1959.000000</td>\n",
       "      <td>1959.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.512506</td>\n",
       "      <td>94.340480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499971</td>\n",
       "      <td>128.717314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>97.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1199.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label       length\n",
       "count  1959.000000  1959.000000\n",
       "mean      0.512506    94.340480\n",
       "std       0.499971   128.717314\n",
       "min       0.000000     2.000000\n",
       "25%       0.000000    28.500000\n",
       "50%       1.000000    47.000000\n",
       "75%       1.000000    97.500000\n",
       "max       1.000000  1199.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add another column with corresponding comment length\n",
    "data_comments['length'] = data_comments['content'].map(lambda text: len(text))\n",
    "\n",
    "# Summary statistics (mean, stdev, min, max)\n",
    "data_comments[[\"label\",\"length\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of evaluation our Naive Bayes classification algorithm, we will split the data into a training and test set. The training set will be used to train the spam classification algorithm, and the test set will only be used to test its accuracy. In general the training set should be bigger than the test set and both have should be drawn from the same population (population in our case is youtube comments for music videos). We will randomly select $75\\%$ of the data as training, and $25\\%$ of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1485.000000\n",
       "mean        0.509764\n",
       "std         0.500073\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         1.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split data into training and test set (75% training, 25% test)\n",
    "\n",
    "# Set seed so we get same random allocation on each run of code\n",
    "np.random.seed(2017)\n",
    "\n",
    "# Add column vector of randomly generated numbers form U[0,1]\n",
    "data_comments[\"uniform\"] = np.random.uniform(0,1,len(data_comments.index)) \n",
    "\n",
    "# About 75% of these numbers should be less than 0.75\n",
    "data_comments_train = data_comments[data_comments[\"uniform\"] < 0.75]\n",
    "\n",
    "# About 25% of these numbers should be more than 0.75\n",
    "data_comments_test = data_comments[data_comments[\"uniform\"] > 0.75]\n",
    "\n",
    "# Check that both training and test data have both spam and ham comments\n",
    "data_comments_train[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    474.000000\n",
       "mean       0.521097\n",
       "std        0.500083\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data summary statistics\n",
    "data_comments_test[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and test data have a good mix spam and ham comments, so we are ready to move onto training the Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training data: 5898\n",
      "First 5 words in our unique set of words: \n",
      " ['He', 'hand,', 'write.', 'posts', 'dance']\n"
     ]
    }
   ],
   "source": [
    "# Join all the comments into a big list\n",
    "training_list_words = \"\".join(data_comments_train.iloc[:,0].values)\n",
    "\n",
    "# Split the list of comments into a list of unique words\n",
    "train_unique_words = set(training_list_words.split(' '))\n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "# Description of summarized comments in training data\n",
    "print('Unique words in training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently \"now!!\" and \"now!!!!\", as well as \"DOES\",\"DoEs\", and \"does\" are all considered to be unique words. For the purposes of spam classification, its probably better to process the data slightly to increase accuracy. In our case we can focus on letters and numbers, as well as convert all the comments to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in processed training data: 4129\n",
      "First 5 words in our processed unique set of words: \n",
      " ['supportcheck', 'nalpure', 'amazed', 'expo', 'insanely']\n"
     ]
    }
   ],
   "source": [
    "# Only keep letters and numbers\n",
    "train_unique_words = [re.sub(r'[^a-zA-Z0-9]','', words) for words in train_unique_words]\n",
    "\n",
    "# Convert to lower case and get unique set of words\n",
    "train_unique_words = set([words.lower() for words in train_unique_words])\n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = len(train_unique_words)\n",
    "\n",
    "# Description of summarized comments in training data\n",
    "print('Unique words in processed training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our processed unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reffering to the Naive Bayes formulation above, in the context of comment spam classification, now $i$ is a index for the comment, the classes $C_{i} = \\{1 = Spam, 0 = Ham\\},$ and features of comments are the words in it, $W_{i}$. For example if the $ith$ comment as \"Check out my chanell!\", then  $W_{i} = \\{Check, out, my, chanell!\\}.$ If we wanted to classify this comment using naive bayes, we need to compute $$Pr(Spam|\\{Check, out, my, chanell!\\}) \\propto Pr(Check|Spam) \\ldots Pr(chanell!|Spam)Pr(Spam),$$ and, $$Pr(Ham|\\{Check, out, my, chanell!\\}) \\propto Pr(Check|Ham) \\ldots Pr(chanell!|Ham)Pr(Ham),$$ \n",
    "note that the proportional symbol is used above since we are omitting $Pr(\\{Check, out, my, chanell!\\})$ in the denominator. \n",
    "\n",
    "Firstly, to find $Pr(Spam)$ and $Pr(Ham)$, we can just compute the proportion of spam and ham emails in our training data respectively. To find the probability of say \"Check\" appearing in spam emails, we can just compute the proportion of times \"Check\" appears in a spam email. That is, $$Pr(Check|Spam) = \\frac{Pr(\\text{Check and Spam})}{Pr(Spam)} = \\frac{\\text{Number of times \"Check\" appers in spam comments}}{\\text{Number of total words in spam comments + Number of unique words in data}}.$$ We can similarly compute $Pr(Check|Ham)$ and also do this with any other word in the comment. Training in Naive Bayes just means we are going to be computing a bunch of conditional probabilities, $Pr(\\text{word in comment}|\\text{comment label}).$ Therefore classification in Naive Bayes is simple as if $$Pr(Spam|Comment) > Pr(Ham|Comment) \\implies \\text{Comment is spam},$$ and if $$Pr(Spam|Comment) < Pr(Ham|Comment) \\implies \\text{Comment is ham}.$$ \n",
    "\n",
    "Given that we calculuate these conditonal probabilities using the words in our training data, what happens when a comment contains words in the training data? The $Pr(\\text{Word not in training data}|Spam) = 0,$ which will imply $Pr(Spam|\\text{Comment with word not in training data}) = 0.$ This is a problem because any spam email with a word not in training data will not be assigned as spam. To resolve this, we compute $$Pr(\\text{word in comment}|Spam) = \\frac{\\alpha+\\text{Number of times word appears in spam comments}}{\\text{Number of total words in spam comments + Number of unique words in data}},$$ where $alpha > 0$ is known as the laplace smoothing parameter. If we say $\\alpha = 1,$ then a $Pr(\\text{Word not in training data}|Spam)$ will be a small positive number instead of $0.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with comment words as \"keys\", and their label as \"value\"\n",
    "trainPositive = dict()\n",
    "trainNegative = dict()\n",
    "\n",
    "# Intiailize classes\n",
    "positiveTotal = 0\n",
    "negativeTotal = 0\n",
    "\n",
    "# Initialize Prob. of\n",
    "pSpam = 0.0\n",
    "pNotSpam = 0.0\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def initialize_dicts():\n",
    "\n",
    "# Initialize dictionary of words and their labels   \n",
    "for word in train_unique_words:\n",
    "    \n",
    "    # Classify all words for now as ham (legitimate)\n",
    "    trainPositive[word] = 0\n",
    "    trainNegative[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of times word in comment appear in spam and ham comments\n",
    "def processComment(comment,label):\n",
    "    global positiveTotal\n",
    "    global negativeTotal\n",
    "    \n",
    "    # Split comments into words\n",
    "    comment = comment.split(' ')\n",
    "    \n",
    "    # Go over each word in comment\n",
    "    for word in comment:\n",
    "        \n",
    "        # ham commments\n",
    "        if(label == 0 and word != ' '):\n",
    "            \n",
    "            # Increment number of times word appears in ham comments\n",
    "            trainNegative[word] = trainNegative.get(word,0)+1\n",
    "            negativeTotal += 1\n",
    "            \n",
    "        # spam comments\n",
    "        elif(label == 1 and word != ' '):\n",
    "            \n",
    "            # Increment number of times word appears in spam comments\n",
    "            trainPositive[word] = trainPositive.get(word,0)+1\n",
    "            positiveTotal += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prob(word|spam) and Prob(word|ham)\n",
    "def conditionalWord(word,label):\n",
    "    \n",
    "    # Laplace smoothing parameter\n",
    "    global alpha\n",
    "    \n",
    "    # word in ham comment\n",
    "    if(label == 0):\n",
    "        # Compute Prob(word|ham)\n",
    "        return (trainNegative.get(word,0)+alpha)/(float)(negativeTotal+alpha*vocab_size_train)\n",
    "    \n",
    "    # word in spam comment\n",
    "    else:\n",
    "        \n",
    "        # Compute Prob(word|ham)\n",
    "        return (trainPositive.get(word,0)+alpha)/(float)(positiveTotal+alpha*vocab_size_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prob(spam|comment) or Prob(ham|comment)\n",
    "def conditionalComment(comment,label):\n",
    "    \n",
    "    # Initialize conditional probability\n",
    "    prob_label_comment = 1.0\n",
    "    \n",
    "    # Split comments into list of words\n",
    "    comment = comment.split(' ')\n",
    "    \n",
    "    # Go through all words in comments\n",
    "    for word in comment:\n",
    "        \n",
    "        # Compute value proportional to Prob(label|comment)\n",
    "        # Conditional indepdence is assumed here\n",
    "        prob_label_comment *= conditionalWord(word,label)\n",
    "    \n",
    "    return prob_label_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train naive bayes by computing several conditional probabilities in training data\n",
    "def train():\n",
    "    \n",
    "    print('Starting training')\n",
    "    global pSpam\n",
    "    global pNotSpam\n",
    "\n",
    "    # Initiailize \n",
    "    total = 0\n",
    "    numNegative = 0\n",
    "    \n",
    "    # Go over each comment in training data\n",
    "    for idx, comment in data_comments_train.iterrows():\n",
    "        \n",
    "        # Comment is ham \n",
    "        if comment.label == 0:\n",
    "            \n",
    "            # Increment ham comment counter\n",
    "            numNegative += 1\n",
    "        \n",
    "        # Increment comment number\n",
    "        total += 1\n",
    "        \n",
    "        # Update dictionary of ham and spam comments\n",
    "        processComment(comment.content,comment.label)\n",
    "    \n",
    "    # Compute prior probabilities, P(spam), P(ham)\n",
    "    pSpam = numNegative/float(total)\n",
    "    pNotSpam = (total - numNegative)/float(total)\n",
    "    \n",
    "    print('Training is now finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Training is now finished\n"
     ]
    }
   ],
   "source": [
    "# Run naive bayes\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify comment are spam or ham\n",
    "def classify(comment):\n",
    "    \n",
    "    global pSpam\n",
    "    global pNotSpam\n",
    "    \n",
    "    # Compute value proportional to Pr(comment|ham)\n",
    "    isNegative = pSpam * conditionalComment(comment,0)\n",
    "    \n",
    "    # Compute value proportional to Pr(comment|spam)\n",
    "    isPositive = pNotSpam * conditionalComment(comment,1)\n",
    "    \n",
    "    # Output True = spam, False = ham\n",
    "    return (isNegative < isPositive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        True\n",
      "4        True\n",
      "16       True\n",
      "25       True\n",
      "35       True\n",
      "40      False\n",
      "41       True\n",
      "49       True\n",
      "51       True\n",
      "58       True\n",
      "62      False\n",
      "64       True\n",
      "67       True\n",
      "68       True\n",
      "69       True\n",
      "76       True\n",
      "81       True\n",
      "84       True\n",
      "85      False\n",
      "86       True\n",
      "90       True\n",
      "94       True\n",
      "97       True\n",
      "99       True\n",
      "100      True\n",
      "104      True\n",
      "106      True\n",
      "109      True\n",
      "110      True\n",
      "114      True\n",
      "        ...  \n",
      "1843     True\n",
      "1846     True\n",
      "1849     True\n",
      "1851    False\n",
      "1859     True\n",
      "1861     True\n",
      "1864    False\n",
      "1866     True\n",
      "1867     True\n",
      "1868     True\n",
      "1873     True\n",
      "1875     True\n",
      "1877     True\n",
      "1879     True\n",
      "1881    False\n",
      "1883     True\n",
      "1886     True\n",
      "1889     True\n",
      "1897     True\n",
      "1907    False\n",
      "1908     True\n",
      "1909     True\n",
      "1910     True\n",
      "1912     True\n",
      "1915     True\n",
      "1917     True\n",
      "1944     True\n",
      "1945     True\n",
      "1948     True\n",
      "1955     True\n",
      "Name: label, Length: 474, dtype: bool\n",
      "Proportion of comments classified correctly on test set: 0.8164556962025317\n"
     ]
    }
   ],
   "source": [
    "# Initialize spam prediction in test data\n",
    "prediction_test = []\n",
    "\n",
    "# Get prediction accuracy on test data\n",
    "for comment in data_comments_test[\"content\"]:\n",
    "\n",
    "    # Classify comment \n",
    "    prediction_test.append(classify(comment))\n",
    "\n",
    "# Check accuracy\n",
    "test_accuracy = np.mean(np.equal(prediction_test, data_comments_test[\"label\"]))\n",
    "print(np.equal(prediction_test, data_comments_test[\"label\"]))\n",
    "#print prediction_test\n",
    "print(\"Proportion of comments classified correctly on test set: %s\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me try writing some comments to see whether they are classified as spam or ham. Recall the \"True\" is for spam comments, and \"False\" is for ham comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spam\n",
    "classify(\"Guys check out my new chanell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spam\n",
    "classify(\"I have solved P vs. NP, check my video https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham\n",
    "classify(\"I liked the video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham\n",
    "classify(\"Its great that this video has so many views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Bag of Words by Using TF-IDF\n",
    "So far we have been using the Bag of Words model to represent comments as vectors. The \"Bag of Words\" is a list of all unique words found in the training data, then each comment can be represented by a vector that contains the frequency of each unique word that appeared in the comment. For example if the training data contains the words $(hi, how, my, grade, are, you),$ then the text \"how are you you\" can be represented by $(0,1,0,0,1,2).$ The main reason we do this in our application is because comments can vary in length, but the length of all unique words stays fixed. \n",
    "\n",
    "In our context, the TF-IDF is a measure of how important a word is in a comment relative to all the words in our training data. For example if a word such as \"the\" appeared in most of the comments, the TF-IDF would be small as this word does not help us differentiate accross comments. Note that \"TF\" stands for \"Term Frequency\", and \"IDF\" stands for \"Inverse Document Frequency\". In particular, \"TF\" denoted by $tf(w,c)$ is the number of times the word $w$ appears in the given comment $c$. Whereas \"IDF\" is a measure of how much information a given word provides in differentiating comments. Specefically, $IDF$ is formulated as $idf(w, D) = log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$ To combine \"TF\" and \"IDF\" together, we simple take the product, hence $$TFIDF = tf(w,c) \\times idf(w, D) = (\\text{Number of times $w$ appears in comment $c$})\\times log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$$\n",
    "Now the $TF-IDF$ can be used to weight the vectors that result from the \"Bag of Words\" approach. For example, suppose a comment contains \"this\" 2 times, hence $tf = 2$. If we then had 1000 comments in our traininig data, and the word \"this\" appears in 100 comments, $idf = log(1000/100) = 2.$ Therefore in this example, the TF-IDF weight would be 2*2 = 4 for the word \"this\" appear twice in a particular comment. To incorprate TF-IDF into the naive bayes setting, we can compute $$Pr(word|spam) = \\frac{\\sum_{\\text{c is spam}}TFIDF(word,c,D)}{\\sum_{\\text{word in spam c}}\\sum_{\\text{c is spam}}TFIDF(word,c,D)+ \\text{Number of unique words in data}},$$ where $TFIDF(word,c,D) = TF(word,c) \\times IDF(word,data).$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute tfidf(word, comment, data)\n",
    "def TFIDF(comment, train):\n",
    "    \n",
    "    # Split comment into list of words\n",
    "    comment = comment.split(' ')\n",
    "    \n",
    "    # Initiailize tfidf for given comment\n",
    "    tfidf_comment = np.zeros(len(comment))\n",
    "    \n",
    "    # Initiailize number of comments containing a word\n",
    "    num_comment_word = 0\n",
    "    \n",
    "    # Intialize index for words in comment\n",
    "    word_index = 0\n",
    "    \n",
    "    # Go over all words in comment\n",
    "    for word in comment:\n",
    "        \n",
    "        # Compute term frequence (tf)\n",
    "        # Count frequency of word in comment\n",
    "        tf = comment.count(word)\n",
    "        \n",
    "        # Find number of comments containing word\n",
    "        for text in train[\"content\"]:\n",
    "            \n",
    "            # Increment word counter if word found in comment\n",
    "            if text.split(' ').count(word) > 0:\n",
    "                num_comment_word += 1\n",
    "        \n",
    "        # Compute inverse document frequency (idf)\n",
    "        # log(Total number of comments/number of comments with word)\n",
    "        idf = np.log(len(train.index)/num_comment_word)\n",
    "        \n",
    "        # Update tf-idf weight for word\n",
    "        tfidf_comment[word_index] = tf*idf\n",
    "        \n",
    "        # Reset comment containing word counter\n",
    "        num_comment_word = 0\n",
    "        \n",
    "        # Move onto next word in comment\n",
    "        word_index += 1\n",
    "        \n",
    "    return tfidf_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.06142304,  1.54111867,  1.84784894,  3.63960841,  3.17603567,\n",
       "        2.04047986,  5.3572599 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF(\"Check out my new music video plz\",data_comments_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduced the Bayes rule and how it is used by the Naive Bayes algorithm for text classification. Using Naive Bayes with bag of words for youtube comment spam classification resulted in about $82 \\%$ accuracy on the training data. Since the model is trained on youtube music videos comments, it may not be very accurate in classifying spam and ham comments for other videos. $\\textbf{TFIDF is in progress, add in accuracy once complete}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
